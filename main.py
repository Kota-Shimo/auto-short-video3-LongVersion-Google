#!/usr/bin/env python
"""
main.py â€“ VOCABå°‚ç”¨ãƒ­ãƒ³ã‚°å‹•ç”»ï¼ˆæ¨ªå‘ã16:9 / ãƒ©ã‚¦ãƒ³ãƒ‰åˆ¶ / æ—¥æœ¬èªTTSæœ€é©åŒ–ï¼‰
- 1ãƒ©ã‚¦ãƒ³ãƒ‰ = Nå˜èªï¼ˆå˜èªâ†’å˜èªâ†’ä¾‹æ–‡Ã—Nï¼‰ + ãã®Nèªã‚’ã™ã¹ã¦å«ã‚€ä¼šè©±
- ãƒ©ã‚¦ãƒ³ãƒ‰ã”ã¨ã«ã€Œå˜èªç¾¤â†’ä¼šè©±ã€ã‚’å…¥ã‚Œã¦æ¬¡ã®å˜èªç¾¤ã¸é€²ã‚€ï¼ˆç¾¤ã”ã¨å¯¾è©±ï¼‰
- ä¾‹æ–‡ã¯å¸¸ã«ã€Œ1æ–‡ã ã‘ã€ã€‚å¤±æ•—æ™‚ã¯æœ€å¤§6å›ã¾ã§å†ç”Ÿæˆã€æœ€å¾Œã¯ãƒ•ã‚§ãƒ¼ãƒ«ã‚»ãƒ¼ãƒ•ã€‚
- ç¿»è¨³ï¼ˆå­—å¹•ï¼‰ã¯1è¡ŒåŒ–ã€è¤‡æ–‡ã¯å…ˆé ­1æ–‡ã®ã¿æ¡ç”¨ã€‚URL/çµµæ–‡å­—/ä½™åˆ†ãªç©ºç™½ã‚’é™¤å»ã€‚
- å˜èªã®ç¿»è¨³ã¯ã€Œä¾‹æ–‡ï¼‹ãƒ†ãƒ¼ãƒï¼‹å“è©ãƒ’ãƒ³ãƒˆã€ã§1èªã«ç¢ºå®šï¼ˆæ–‡è„ˆè¨³ï¼‰ã€‚
- ç”ŸæˆéŸ³å£°ã¯æ¨ªå‘ãã«æœ€é©åŒ–ã•ã‚ŒãŸæœ¬ç‰©ã® 1920x1080 ã‚­ãƒ£ãƒ³ãƒã‚¹ä¸Šã§ãƒ¬ãƒ³ãƒ€ï¼ˆé»’å¸¯ãªã—ï¼‰ã€‚
- â˜… é‡è¦ï¼ško/ja/zh ã§ã¯è‹±å­—æ··å…¥ã‚’ç¦æ­¢ï¼ˆè‹±å­—ãŒå‡ºãŸã‚‰å†ç”Ÿæˆ / TTSç›´å‰ã§è‹±å­—é™¤å»ï¼‰
"""

import argparse, logging, re, json, subprocess, os, sys
from datetime import datetime
from pathlib import Path
from shutil import rmtree

import random  # â† import å¿˜ã‚Œãšã«ï¼
import yaml
from pydub import AudioSegment
from openai import OpenAI

from config         import BASE, OUTPUT, TEMP
from translate      import translate
# --- TTS åˆ‡æ›¿ï¼ˆGoogle / OpenAIï¼‰---
USE_GOOGLE_TTS = os.getenv("USE_GOOGLE_TTS", "0") == "1"
if USE_GOOGLE_TTS:
    from tts_google import speak
else:
    from tts_openai import speak

from audio_fx       import enhance
from bg_image       import fetch as fetch_bg
from thumbnail      import make_thumbnail
from upload_youtube import upload
from topic_picker   import pick_by_content_type

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
GPT = OpenAI()
CONTENT_MODE = "vocab"
DEBUG_SCRIPT = os.getenv("DEBUG_SCRIPT", "0") == "1"

# ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªçµåˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
GAP_MS       = int(os.getenv("GAP_MS", "120"))
PRE_SIL_MS   = int(os.getenv("PRE_SIL_MS", "120"))
MIN_UTTER_MS = int(os.getenv("MIN_UTTER_MS", "1000"))

# â˜… æ—¥æœ¬èªã ã‘å€‹åˆ¥èª¿æ•´
GAP_MS_JA       = int(os.getenv("GAP_MS_JA", str(GAP_MS)))
PRE_SIL_MS_JA   = int(os.getenv("PRE_SIL_MS_JA", str(PRE_SIL_MS)))
MIN_UTTER_MS_JA = int(os.getenv("MIN_UTTER_MS_JA", "800"))

# ç”Ÿæˆæ¸©åº¦
EX_TEMP_DEFAULT = float(os.getenv("EX_TEMP", "0.35"))   # ä¾‹æ–‡
LIST_TEMP       = float(os.getenv("LIST_TEMP", "0.30")) # èªå½™ãƒªã‚¹ãƒˆ

# èªå½™ãƒ»ãƒ©ã‚¦ãƒ³ãƒ‰
VOCAB_WORDS   = int(os.getenv("VOCAB_WORDS", "6"))      # 1ãƒ©ã‚¦ãƒ³ãƒ‰ã®å˜èªæ•°
VOCAB_ROUNDS  = int(os.getenv("VOCAB_ROUNDS", "1"))     # ãƒ©ã‚¦ãƒ³ãƒ‰æ•°
CONVO_LINES   = int(os.getenv("CONVO_LINES", "15"))      # ãã®ãƒ©ã‚¦ãƒ³ãƒ‰æœ«ã®ä¼šè©±è¡Œæ•°ï¼ˆå¶æ•°æ¨å¥¨ï¼‰
# â† æ—¢å­˜: VOCAB_WORDS / VOCAB_ROUNDS / CONVO_LINES ã®å®šç¾©ã®ä¸‹ã‚ãŸã‚Šã«è¿½åŠ 
WORD_REPEAT = int(os.getenv("WORD_REPEAT", "1"))  # æ—¢å®š=2å›ï¼ˆå¾“æ¥äº’æ›ï¼‰
NO_CONVO    = os.getenv("NO_CONVO", "0") == "1"   # 1ã§ä¼šè©±ãƒ–ãƒ­ãƒƒã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—

# æ¨ªå‘ã 16:9 ãƒ¬ãƒ³ãƒ€è¨­å®šï¼ˆchunk_builder ã«æ¸¡ã™ï¼‰
RENDER_SIZE   = os.getenv("RENDER_SIZE", "1920x1080")
RENDER_BG_FIT = os.getenv("RENDER_BG_FIT", "cover")

LANG_NAME = {
    "en": "English", "pt": "Portuguese", "id": "Indonesian",
    "ja": "Japanese","ko": "Korean", "es": "Spanish", "fr": "French",
    "zh": "Chinese",
}

# ====== â€œå‡ºãŒã¡ã®å®‰å…¨ã™ãã‚‹èªâ€ã‚’æŠ‘åˆ¶ï¼ˆãƒ†ãƒ¼ãƒç„¡é–¢é€£åŒ–ã‚’é¿ã‘ã‚‹ï¼‰ ======
BANNED_COMMON_BY_LANG = {
    "ja": {"ãƒã‚§ãƒƒã‚¯ã‚¤ãƒ³", "ãƒã‚§ãƒƒã‚¯ã‚¢ã‚¦ãƒˆ", "äºˆç´„", "é ˜åæ›¸", "ãƒ¬ã‚·ãƒ¼ãƒˆ", "ãƒ­ãƒ“ãƒ¼", "ã‚¨ãƒ¬ãƒ™ãƒ¼ã‚¿ãƒ¼", "ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰", "å®¢å®¤", "éƒ¨å±‹"},
    "ko": {"ì²´í¬ì¸", "ì²´í¬ì•„ì›ƒ", "ì˜ˆì•½", "ì˜ìˆ˜ì¦", "ë¡œë¹„", "ì—˜ë¦¬ë² ì´í„°", "ì—…ê·¸ë ˆì´ë“œ", "ê°ì‹¤"},
    "zh": {"åŠç†å…¥ä½", "é€€æˆ¿", "é¢„è®¢", "å‘ç¥¨", "å¤§å ‚", "ç”µæ¢¯", "å‡çº§", "æˆ¿é—´"},
    "es": {"registro", "reserva", "salida", "recibo", "ascensor", "vestÃ­bulo", "mejora"},
    "pt": {"check-in", "reserva", "checkout", "recibo", "elevador", "saguÃ£o", "upgrade"},
    "fr": {"enregistrement", "rÃ©servation", "dÃ©part", "reÃ§u", "ascenseur", "hall", "surclassement"},
    "id": {"check-in", "reservasi", "check-out", "struk", "lift", "lobi", "upgrade"},
    "en": {"check-in", "reservation", "checkout", "receipt", "elevator", "lobby", "upgrade"},
}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CEFR é›£æ˜“åº¦ã®æ±ºå®šï¼ˆå‹•ç”»å†…ã§å›ºå®šãƒ»å‹•ç”»ã”ã¨ã«ãƒ©ãƒ³ãƒ€ãƒ å¯ï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_VALID_CEFR = {"A1","A2","B1","B2"}

def _pick_difficulty_for_video() -> str:
    explicit = os.getenv("CEFR_LEVEL", "").strip().upper()
    if explicit in _VALID_CEFR:
        return explicit

    pool_env = os.getenv("CEFR_POOL", "A1,A2,B1,B2")
    pool = [x.strip().upper() for x in pool_env.split(",") if x.strip()]
    pool = [x for x in pool if x in _VALID_CEFR] or ["A1","A2","B1","B2"]

    seed_env = os.getenv("VIDEO_SEED", "").strip()
    if seed_env:
        try:
            random.seed(int(seed_env))
        except Exception:
            random.seed(seed_env)

    return random.choice(pool)

def _banned_for(lang_code: str) -> set[str]:
    return set(BANNED_COMMON_BY_LANG.get(lang_code, set()))

JP_CONV_LABEL = {
    "en": "è‹±ä¼šè©±", "ja": "æ—¥æœ¬èªä¼šè©±", "es": "ã‚¹ãƒšã‚¤ãƒ³èªä¼šè©±",
    "pt": "ãƒãƒ«ãƒˆã‚¬ãƒ«èªä¼šè©±", "ko": "éŸ“å›½èªä¼šè©±", "id": "ã‚¤ãƒ³ãƒ‰ãƒã‚·ã‚¢èªä¼šè©±",
    "fr": "ãƒ•ãƒ©ãƒ³ã‚¹èªä¼šè©±", "zh": "ä¸­å›½èªä¼šè©±",
}

with open(BASE / "combos.yaml", encoding="utf-8") as f:
    COMBOS = yaml.safe_load(f)["combos"]

# ä¾‹æ–‡ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯çµ±è¨ˆ
FALLBACK_STATS = {"example_attempts": 0, "example_fallbacks": 0}

def reset_temp():
    if TEMP.exists():
        rmtree(TEMP)
    TEMP.mkdir(exist_ok=True)

def sanitize_title(raw: str) -> str:
    """
    ã‚¿ã‚¤ãƒˆãƒ«ã¯â€œåˆ‡ã‚‰ãªã„â€æ–¹é‡ï¼š
      - ä½™è¨ˆãªç•ªå·ãƒ»å…¨è§’ç©ºç™½ãªã©ã®ãƒã‚¤ã‚ºã ã‘é™¤å»
      - YouTubeä¸Šé™(100æ–‡å­—)ã¯é™ã‹ã«ã‚«ãƒƒãƒˆï¼ˆçœç•¥è¨˜å·ã¯ä»˜ã‘ãªã„ï¼‰
    """
    title = re.sub(r"^\s*(?:\d+\s*[.)]|[-â€¢ãƒ»])\s*", "", raw)  # å…ˆé ­ã®ç•ªå·ã‚„è¨˜å·
    title = re.sub(r"[\s\u3000]+", " ", title).strip()       # é€£ç¶šã‚¹ãƒšãƒ¼ã‚¹æ­£è¦åŒ–
    return title[:100]  # çœç•¥è¨˜å·ã¯ä»˜ã‘ãªã„

def _infer_title_lang(audio_lang: str, subs: list[str], combo: dict) -> str:
    if "title_lang" in combo and combo["title_lang"]:
        return combo["title_lang"]
    if len(subs) >= 2:
        return subs[1]
    for s in subs:
        if s != audio_lang:
            return s
    return audio_lang

def resolve_topic(arg_topic: str) -> str:
    return arg_topic

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ãƒ»ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_URL_RE   = re.compile(r"https?://\S+")
_NUM_LEAD = re.compile(r"^\s*\d+[\).:\-]\s*")
_QUOTES   = re.compile(r'^[\"â€œâ€\']+|[\"â€œâ€\']+$')
_EMOJI_RE = re.compile(r"[\U00010000-\U0010ffff]")
_SENT_END = re.compile(r"[ã€‚.!?ï¼ï¼Ÿ]")

def _normalize_spaces(t: str) -> str:
    return re.sub(r"\s+", " ", (t or "")).strip()

def _clean_strict(text: str) -> str:
    t = (text or "").strip()
    t = _URL_RE.sub("", t)
    t = _NUM_LEAD.sub("", t)
    t = _QUOTES.sub("", t)
    t = _EMOJI_RE.sub("", t)
    t = re.sub(r"[\:\-â€“â€”]\s*$", "", t)
    return _normalize_spaces(t)

def _is_single_sentence(text: str) -> bool:
    return len(_SENT_END.findall(text or "")) <= 1

def _fits_length(text: str, lang_code: str) -> bool:
    if lang_code in ("ja", "ko", "zh"):
        return len(text or "") <= 30
    return len(re.findall(r"\b\w+\b", text or "")) <= 12

def _ensure_period_for_sentence(txt: str, lang_code: str) -> str:
    t = txt or ""
    return t if re.search(r"[ã€‚.!?ï¼ï¼Ÿ]$", t) else t + ("ã€‚" if lang_code == "ja" else ".")

def _clean_sub_line(text: str, lang_code: str) -> str:
    t = _clean_strict(text).replace("\n", " ").strip()
    m = _SENT_END.search(t)
    if m:
        t = t[:m.end()]
    return t

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ãƒ¢ãƒãƒªãƒ³ã‚¬ãƒ«å¼·åˆ¶ï¼ˆko/ja/zhã¯è‹±å­—ç¦æ­¢ï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_ASCII_LETTERS = re.compile(r"[A-Za-z]")

def _monolingual_ok(text: str, lang_code: str) -> bool:
    """ko/ja/zh ã¯è‹±å­—ã‚’å«ã¾ãªã„ã“ã¨ã€‚ãã®ä»–ã¯è‡ªç”±ï¼ˆLatinç³»è¨€èªã®ãŸã‚ï¼‰ã€‚"""
    if lang_code in ("ko", "ja", "zh"):
        return not _ASCII_LETTERS.search(text or "")
    return True

# TTSç›´å‰ã®éè‹±èªã‚¢ã‚¹ã‚­ãƒ¼é™¤å»ï¼ˆå®‰å…¨å´ï¼‰
def _purge_ascii_for_tts(text: str, lang_code: str) -> str:
    if lang_code in ("ko", "ja", "zh"):
        t = re.sub(r"[A-Za-z]+", "", text or "")
        t = re.sub(r"\s{2,}", " ", t).strip()
        return t or (text or "")
    return text or ""

# è‹±èªä»¥å¤–ã§ç´›ã‚Œè¾¼ã‚“ã è‹±å˜èªã‚’å¼±ã‚ã‚‹è»½ã„ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆæ—¢å­˜äº’æ›ï¼‰
_LATIN_WORD_RE = re.compile(r"\b[A-Za-z]{3,}\b")
def _clean_non_english_ascii(text: str, lang_code: str) -> str:
    """
    ğŸ¯ ä¿®æ­£ç‰ˆï¼š
    - éŸ“å›½èªãƒ»æ—¥æœ¬èªãƒ»ä¸­å›½èªã ã‘è‹±å­—é™¤å»
    - ã‚¹ãƒšã‚¤ãƒ³èªãƒ»ãƒ•ãƒ©ãƒ³ã‚¹èªãƒ»ãƒãƒ«ãƒˆã‚¬ãƒ«èªãªã©ãƒ©ãƒ†ãƒ³æ–‡å­—è¨€èªã¯ãã®ã¾ã¾ä¿æŒ
    """
    # ko / ja / zh ã®ã¿è‹±å­—é™¤å»
    if lang_code in ("ko", "ja", "zh"):
        return _purge_ascii_for_tts(text, lang_code)
    # ãã‚Œä»¥å¤–ï¼ˆes, pt, fr, idãªã©ï¼‰ã¯è§¦ã‚‰ãªã„
    return text

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# æ—¥æœ¬èªTTSæœ€é©åŒ–
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_KANJI_ONLY = re.compile(r"^[ä¸€-é¾¥ã€…]+$")
_PARENS_JA  = re.compile(r"\s*[\(\ï¼ˆ][^)\ï¼‰]{1,40}[\)\ï¼‰]\s*")

def _to_kanji_digits(num_str: str) -> str:
    table = str.maketrans("0123456789", "ã€‡ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹")
    return num_str.translate(table)

def normalize_ja_for_tts(text: str) -> str:
    t = text or ""
    t = re.sub(r"[\(ï¼ˆ][^)\ï¼‰]{1,40}[\)ï¼‰]", "", t)
    t = t.replace("/", "ã€").replace("-", "ã€").replace(":", "ã€").replace("ãƒ» ãƒ»", "ãƒ»")
    t = re.sub(r"\d{1,}", lambda m: _to_kanji_digits(m.group(0)), t)
    t = re.sub(r"([A-Za-z]{2,})", lambda m: "ãƒ»".join(list(m.group(1).lower())), t)
    t = re.sub(r"[ã€‚]{2,}", "ã€‚", t)
    t = re.sub(r"[ã€]{2,}", "ã€", t)
    t = re.sub(r"\s{2,}", " ", t).strip()
    if t and t[-1] not in "ã€‚ï¼ï¼Ÿ!?":
        t += "ã€‚"
    return t

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ç¿»è¨³å¼·åŒ–
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_ASCII_ONLY = re.compile(r'^[\x00-\x7F]+$')

def _looks_like_english(s: str) -> bool:
    s = (s or "").strip()
    return bool(_ASCII_ONLY.fullmatch(s)) and bool(re.search(r'[A-Za-z]', s))

def _needs_retranslate(output: str, src_lang: str, target_lang: str, original: str) -> bool:
    if target_lang == src_lang:
        return False
    out = (output or "").strip()
    if not out:
        return True
    if out.lower() == (original or "").strip().lower():
        return True
    if target_lang == "en" and not _looks_like_english(out):
        return True
    return False

def translate_sentence_strict(sentence: str, src_lang: str, target_lang: str) -> str:
    try:
        first = translate(sentence, target_lang)
    except Exception:
        first = ""
    if not _needs_retranslate(first, src_lang, target_lang, sentence):
        return _clean_sub_line(first, target_lang)
    try:
        _ = GPT.chat_completions.create
    except AttributeError:
        pass
    try:
        rsp = GPT.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{
                "role":"user",
                "content":(
                    f"Translate from {LANG_NAME.get(src_lang,'source language')} "
                    f"to {LANG_NAME.get(target_lang,'target language')}.\n"
                    "Return ONLY the translation as a single sentence. "
                    "No explanations, no quotes, no extra symbols.\n\n"
                    f"Text: {sentence}"
                )
            }],
            temperature=0.0, top_p=1.0
        )
        out = (rsp.choices[0].message.content or "").strip()
        out = _clean_sub_line(out, target_lang)
        if out:
            return out
    except Exception:
        pass
    return _clean_sub_line(sentence, target_lang)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ãƒ©ãƒ³ã‚°ã‚¨ãƒ¼ã‚¸ãƒ«ãƒ¼ãƒ«ï¼ˆå³å¯†ãƒ¢ãƒãƒªãƒ³ã‚¬ãƒ«ï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _lang_rules(lang_code: str) -> str:
    if lang_code == "ja":
        return (
            "Write entirely in Japanese. "
            "Do not include Latin letters or other languages. "
            "Avoid ASCII symbols such as '/', '-', 'â†’', '()', '[]', '<>', and '|'. "
            "No translation glosses, brackets, or country/language mentions."
        )
    if lang_code == "ko":
        return (
            "Write entirely in Korean (Hangul only). "
            "Do not include Latin letters or other languages. "
            "Avoid ASCII symbols such as '/', '-', 'â†’', '()', '[]', '<>', and '|'. "
            "No translation glosses or stage directions."
        )
    if lang_code == "zh":
        return (
            "Write entirely in Chinese. "
            "Do not include Latin letters or other languages. "
            "Avoid ASCII symbols such as '/', '-', 'â†’', '()', '[]', '<>', and '|'. "
            "No translation glosses or stage directions."
        )
    lang_name = LANG_NAME.get(lang_code, "English")
    return (
        f"Write entirely in {lang_name}. "
        "Do not code-switch or include other writing systems. "
        "Avoid ASCII symbols like '/', '-', 'â†’', '()', '[]', '<>', and '|'. "
        "No translation glosses, brackets, or country/language mentions."
    )

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# æ—¥æœ¬èª fallback
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _guess_ja_pos(word: str) -> str:
    w = (word or "").strip()
    if not w:
        return "noun"
    if w.endswith(("ã™ã‚‹", "ã—ã¾ã™", "ã—ãŸã„", "ã—ãŸ", "ã—ãªã„", "ã—ã‚ˆã†")):
        return "verb"
    if re.search(r"(ã†|ã|ã|ã™|ã¤|ã¬|ã‚€|ã¶|ã‚‹)$", w):
        return "verb"
    if w.endswith("ã„"):
        return "iadj"
    if w.endswith(("çš„", "çš„ãª", "é¢¨")):
        return "naadj"
    if re.fullmatch(r"[ã‚¡-ãƒ¶ãƒ¼]+", w):
        return "noun"
    return "noun"

def _ja_template_fallback(word: str) -> str:
    kind = _guess_ja_pos(word)
    if kind == "verb":
        return f"{word}ã¨ã“ã‚ã§ã™ã€‚"
    if kind == "iadj":
        return f"{word}ã§ã™ã­ã€‚"
    if kind == "naadj":
        return f"{word}ã ã­ã€‚"
    return f"{word}ãŒå¿…è¦ã§ã™ã€‚"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# èªå½™ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _example_temp_for(lang_code: str) -> float:
    return 0.20 if lang_code == "ja" else EX_TEMP_DEFAULT

_WIDE_DASH = re.compile(r"[â€-â€’â€“â€”]")

def _contains_word_relaxed(word: str, cand: str, lang_code: str) -> bool:
    """
    ja/ko/zh ã¯æ´»ç”¨ãƒ»æ´¾ç”Ÿãƒ»é€ã‚Šä»®åå·®ã‚’è¨±å®¹ã—ã¦ 'å«ã‚€' ã¨ã¿ãªã™ã€‚
    ä»–è¨€èªã¯å¤§æ–‡å­—å°æ–‡å­—ç„¡è¦–ã®å˜ç´”éƒ¨åˆ†ä¸€è‡´ã€‚
    """
    if not word or not cand:
        return False
    t = _WIDE_DASH.sub("-", cand)

    if lang_code in ("ja", "ko", "zh"):
        w = word.strip()
        # å®Œå…¨ä¸€è‡´/éƒ¨åˆ†ä¸€è‡´
        if w in t:
            return True
        # JA: ã™ã‚‹å‹•è©
        if lang_code == "ja" and w.endswith("ã™ã‚‹"):
            stem = w[:-2]
            if stem and (stem in t or f"{stem}ã—" in t):
                return True
        # JA: æ¼¢å­—2æ–‡å­—ä»¥ä¸Šã®é€£ç¶šä¸€è‡´
        kanjis = "".join(ch for ch in w if "\u4e00" <= ch <= "\u9fff" or ch == "ã€…")
        if len(kanjis) >= 2 and kanjis in t:
            return True
        # KO: í•˜ë‹¤ ã®èªå¹¹ã‚†ã‚‹ä¸€è‡´
        if lang_code == "ko" and w.endswith("í•˜ë‹¤"):
            base = w[:-2]
            if base and base in t:
                return True
        # ZH: äº†/è‘—/è¿‡ ã®ä»˜éšè¨±å®¹
        if lang_code == "zh":
            if w in t or (w + "äº†") in t or (w + "è‘—") in t or (w + "è¿‡") in t:
                return True
        # ç©ºç™½é™¤å»ã§å†ãƒã‚§ãƒƒã‚¯
        if w.replace(" ", "") and w.replace(" ", "") in t.replace(" ", ""):
            return True
        return False

    # Latin ç³»ï¼šå˜ç´”éƒ¨åˆ†ä¸€è‡´ï¼ˆå¤§å°ç„¡è¦–ï¼‰
    return (word or "").lower() in t.lower()

def _gen_example_sentence(word: str, lang_code: str, context_hint: str = "", difficulty: str | None = None) -> str:
    lang_name = LANG_NAME.get(lang_code, "English")
    ctx = (context_hint or "").strip()
    rules = _lang_rules(lang_code)
    system = {
        "role": "system",
        "content": (
            "You write exactly ONE natural sentence. "
            "No lists, no quotes, no emojis, no URLs. Keep it monolingual."
        ),
    }

    # é›£æ˜“åº¦æŒ‡ç¤º
    level_line = f" CEFR {difficulty} level ã®é›£æ˜“åº¦ã«ã—ã¦ãã ã•ã„ã€‚" if difficulty else ""

    if lang_code == "ja":
        user = (
            f"{rules} "
            f"å˜èªã€Œ{word}ã€ã‚’å¿…ãšå«ã‚ã¦ã€æ—¥æœ¬èªã§è‡ªç„¶ãªä¸€æ–‡ã‚’ã¡ã‚‡ã†ã©1ã¤ã ã‘æ›¸ã„ã¦ãã ã•ã„ã€‚"
            "æ—¥å¸¸ã®ç°¡å˜ãªçŠ¶æ³ã‚’æƒ³å®šã—ã€åŠ©è©ã®ä½¿ã„æ–¹ã‚’è‡ªç„¶ã«ã—ã¦ãã ã•ã„ã€‚"
            "ã‹ã£ã“æ›¸ãã‚„ç¿»è¨³æ³¨é‡ˆã¯ä¸è¦ã§ã™ã€‚è‹±å­—ã¯ç¦æ­¢ã€‚"
            "å¯èƒ½ã§ã‚ã‚Œã°è¦‹å‡ºã—èªã«è¿‘ã„å½¢ã§ä½¿ã£ã¦ãã ã•ã„ï¼ˆãŸã ã—ä¸è‡ªç„¶ãªã‚‰æ´»ç”¨ã—ã¦æ§‹ã„ã¾ã›ã‚“ï¼‰ã€‚"
            f"{level_line}"
        )
        if ctx:
            user += f" ã‚·ãƒ¼ãƒ³ã®æ–‡è„ˆ: {ctx}"

    elif lang_code == "ko":
        user = (
            f"{rules} "
            f"ë‹¤ìŒ ë‹¨ì–´ë¥¼ ë°˜ë“œì‹œ í¬í•¨í•˜ì—¬ í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ì„ ì •í™•íˆ 1ê°œë§Œ ì“°ì„¸ìš”: {word} "
            "ëŒ€ê´„í˜¸ë‚˜ ë²ˆì—­ ë©”ëª¨ ê¸ˆì§€. ì˜ë¬¸ì ì‚¬ìš© ê¸ˆì§€."
            "ê°€ëŠ¥í•˜ë©´ ê¸°ë³¸í˜•ì— ê°€ê¹ê²Œ ì“°ë˜, ë¶€ìì—°ìŠ¤ëŸ¬ìš°ë©´ í™œìš©í•´ë„ ë©ë‹ˆë‹¤."
            f"{level_line}"
        )
        if ctx:
            user += f" ì¥ë©´ íŒíŠ¸: {ctx}"

    elif lang_code == "zh":
        user = (
            f"{rules} "
            f"å¿…é¡»åŒ…å«è¯¥è¯ï¼Œå¹¶åªå†™ä¸€å¥è‡ªç„¶çš„å¥å­ï¼š{word}ã€‚"
            "ä¸è¦ä½¿ç”¨æ‹¬å·æˆ–ç¿»è¯‘æ³¨é‡Šã€‚ä¸è¦ä½¿ç”¨æ‹‰ä¸å­—æ¯ã€‚"
            "è‹¥å¯èƒ½è¯·ä½¿ç”¨è¯å…¸å½¢å¼ï¼Œè‹¥ä¸è‡ªç„¶å¯ä»¥é€‚åº¦å˜åŒ–ã€‚"
            f"{level_line}"
        )
        if ctx:
            user += f" åœºæ™¯æç¤ºï¼š{ctx}"

    else:
        user = (
            f"{rules} "
            f"Write exactly ONE short, natural sentence in {lang_name} that uses the word: {word}. "
            "Return ONLY the sentence. Prefer using the dictionary form if natural."
            f"{level_line}"
        )
        if ctx:
            user += f" Scene hint: {ctx}"

    for _ in range(6):
        try:
            rsp = GPT.chat.completions.create(
                model="gpt-4o-mini",
                messages=[system, {"role": "user", "content": user}],
                temperature=_example_temp_for(lang_code),
                top_p=0.9,
            )
            raw = (rsp.choices[0].message.content or "").strip()
        except Exception:
            raw = ""
        cand = _clean_strict(raw)
        FALLBACK_STATS["example_attempts"] += 1
        if not _monolingual_ok(cand, lang_code):
            continue
        valid = bool(cand) and _is_single_sentence(cand) and _fits_length(cand, lang_code)
        try:
            contains_word = _contains_word_relaxed(word, cand, lang_code)
        except Exception:
            contains_word = True
        if valid and contains_word:
            return _ensure_period_for_sentence(cand, lang_code)

    # ãƒ•ã‚§ãƒ¼ãƒ«ã‚»ãƒ¼ãƒ•ï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä¾‹æ–‡ï¼‰
    FALLBACK_STATS["example_fallbacks"] += 1
    if lang_code == "ja":
        return _ja_template_fallback(word)
    elif lang_code == "ko":
        return _ensure_period_for_sentence(f"{word}ë¥¼ ì—°ìŠµí•´ ë´…ì‹œë‹¤", lang_code)
    elif lang_code == "zh":
        return _ensure_period_for_sentence(f"è®©æˆ‘ä»¬ç»ƒä¹ {word}", lang_code)
    elif lang_code == "es":
        return _ensure_period_for_sentence(f"Practiquemos {word}", lang_code)
    elif lang_code == "pt":
        return _ensure_period_for_sentence(f"Vamos praticar {word}", lang_code)
    elif lang_code == "fr":
        return _ensure_period_for_sentence(f"Pratiquons {word}", lang_code)
    elif lang_code == "id":
        return _ensure_period_for_sentence(f"Ayo berlatih {word}", lang_code)
    else:
        return _ensure_period_for_sentence(f"Let's practice {word}", lang_code)

def _extract_words(text: str, lang_code: str, n: int, banned: set[str]) -> list[str]:
    """
    å³å¯†ãƒ‘ãƒ¼ã‚µï¼šè¡Œé ­ç•ªå·é™¤å»/å¥èª­ç‚¹é™¤å»/ã‚¹ã‚¯ãƒªãƒ—ãƒˆè¦å‰‡/é‡è¤‡é™¤å»/ãƒãƒ³ãƒªã‚¹ãƒˆé™¤å»ã€‚
    Latinç³»ã¯æœ€åˆã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿æ¡ç”¨ï¼ˆ"credit card" â†’ "credit"ï¼‰â€»å¿…è¦ãªã‚‰ hyphen ã¯è¨±å¯ã€‚
    """
    if not text:
        return []
    lines = [ (ln or "").strip() for ln in text.splitlines() ]
    out: list[str] = []
    seen: set[str] = set()
    for ln in lines:
        if not ln:
            continue
        # å…ˆé ­ç•ªå·ãƒ»æ¥é ­å¥ã‚’é™¤å»
        ln = re.sub(r"^\s*(?:[-â€¢ãƒ»]|\d+[\).:]?)\s*", "", ln)
        # æœ«å°¾ã®å¥èª­ç‚¹ã‚„è£…é£¾ã‚’é™¤å»
        ln = re.sub(r"[ï¼Œã€ã€‚.!?ï¼ï¼Ÿâ€¦:;]+$", "", ln).strip()
        if not ln:
            continue

        # è¨€èªåˆ¥ã‚¹ã‚¯ãƒªãƒ—ãƒˆåˆ¶ç´„
        if lang_code in ("ja", "ko", "zh"):
            # è‹±å­—æ··å…¥ã‚’å¼¾ã
            if _ASCII_LETTERS.search(ln):
                continue
            w = ln.replace("ã€€", " ").split()[0]  # ä¸‡ä¸€ã‚¹ãƒšãƒ¼ã‚¹ãŒã‚ã‚Œã°å…ˆé ­ãƒˆãƒ¼ã‚¯ãƒ³
        else:
            # Latinç³»ã¯1ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼ˆhyphenã¯è¨±å®¹ï¼‰
            token = ln.split()[0]
            token = re.sub(r"[^\w\-â€™']", "", token)
            w = token

        if not w:
            continue

        # çŸ­ã™ã/é•·ã™ã/æ•°å­—ã®ã¿ ã‚’é™¤å¤–
        if len(w) < 2 or len(w) > 24 or re.fullmatch(r"\d+", w):
            continue

        # ãƒãƒ³ãƒªã‚¹ãƒˆãƒ»é‡è¤‡
        key = (w.lower() if lang_code not in ("ja", "ko", "zh") else w)
        if w in banned or key in seen:
            continue

        out.append(w)
        seen.add(key)
        if len(out) >= n:
            break
    return out

def _gen_more_words_excluding(theme_for_prompt: str, lang_code: str, need: int, exclude: list[str], diff_hint: str = "") -> str:
    """
    ä¸è¶³åˆ†ã®ã¿è¿½åŠ å–å¾—ã€‚ã™ã§ã«å¾—ãŸèª(exclude)ã¨ãƒãƒ³ãƒªã‚¹ãƒˆã‚’æ˜ç¤ºã—ã¦ç”Ÿæˆã€‚
    """
    lang_name = LANG_NAME.get(lang_code, "the target language")
    banned = sorted(_banned_for(lang_code) | set(exclude))
    banned_line = ", ".join(banned[:50])  # é•·ã™ãå›é¿

    lines = [
        f"List {need} HIGH-FREQUENCY words for: {theme_for_prompt}.",
        f"Language: {lang_name}. Return ONLY one word per line, no numbering.",
        "No explanations. No examples. No punctuation.",
        f"Do NOT include any of these words: {banned_line or '(none)'}."
    ]
    if diff_hint:
        lines.append(f"Approximate CEFR level: {diff_hint}. Prefer common, practical words.")
    if lang_code in ("ko","ja","zh"):
        lines.append("Use ONLY the target script. Do not use Latin letters.")
    return "\n".join(lines)

def _gen_vocab_list(theme: str, lang_code: str, n: int) -> list[str]:
    """
    æ”¹è‰¯ç‰ˆ:
      - GPTãŒç©ºè¿”ã—ã‚„è‹±å­—æ··å…¥ã‚’èµ·ã“ã•ãªã„ã‚ˆã†ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ˜ç¢ºåŒ–
      - ãƒªãƒˆãƒ©ã‚¤æœ€å¤§5å›ã€é–“éš”1ç§’
      - 3å›å¤±æ•—ã—ãŸã‚‰ä¸Šä½ãƒ¢ãƒ‡ãƒ«(gpt-4o)ã§å†å®Ÿè¡Œ
      - ãƒ­ã‚°å‡ºåŠ›ã§åŸå› è¿½è·¡
      - å‡ºåŠ›ãŒç©ºã®å ´åˆã‚‚ã€Œè‡ªç„¶èªï¼‹å¤šè¨€èªå¯¾å¿œãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã€
    """
    import time, random
    theme_for_prompt = translate(theme, lang_code) if lang_code != "en" else theme

    prompt = (
        f"List exactly {n} essential, real, single words commonly used in the topic: {theme_for_prompt}. "
        "Write only one word per line. No numbering, no punctuation, no explanations.\n"
        "All words must be natural and commonly used by native speakers. "
        "If the language uses non-Latin characters (e.g., Japanese, Korean, Chinese), use native script only. "
        "If transliteration exists, prefer native writing. Output only the list."
    )

    content = ""
    for attempt in range(5):
        try:
            model_name = "gpt-4o-mini" if attempt < 3 else "gpt-4o"
            rsp = GPT.chat.completions.create(
                model=model_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.4,
                top_p=0.9,
            )
            content = (rsp.choices[0].message.content or "").strip()
            if content:
                break
        except Exception as e:
            print(f"[WARN] vocab generation failed ({lang_code}, try {attempt+1}/5): {e}")
        time.sleep(1)

    # äºŒæ®µéšå†è©¦è¡Œï¼ˆå®Œå…¨ç©ºã®ã¨ãï¼‰
    if not content.strip():
        try:
            print(f"[INFO] second-phase retry for {lang_code}")
            rsp = GPT.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": prompt + "\nRepeat your answer clearly."}],
                temperature=0.5,
            )
            content = (rsp.choices[0].message.content or "").strip()
        except Exception:
            content = ""

    # ãƒ‘ãƒ¼ã‚¹ã¨æ•´å½¢
    lines = [l.strip("ãƒ»-â€”â€¢ ").strip() for l in content.splitlines() if l.strip()]
    lines = [re.sub(r"^\d+[.)] ?", "", l) for l in lines if l]
    words = [l for l in lines if len(l) <= 20]

    # fallback: æ±ç”¨èªãƒªã‚¹ãƒˆ
    if len(words) < n:
        print(f"[FALLBACK] insufficient vocab for {theme} ({lang_code}), got {len(words)}")
        FALLBACKS = {
            "ja": ["ãƒ›ãƒ†ãƒ«", "æ—…è¡Œ", "ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³", "è²·ã„ç‰©", "ç©ºæ¸¯", "é£Ÿã¹ç‰©", "æ™‚é–“", "åœ°å›³", "æ”¯æ‰•ã„"],
            "ko": ["í˜¸í…”", "ì—¬í–‰", "ì‹ë‹¹", "ì‡¼í•‘", "ê³µí•­", "ìŒì‹", "ì‹œê°„", "ì§€ë„", "ê²°ì œ"],
            "zh": ["é…’åº—", "æ—…è¡Œ", "é¤å…", "è´­ç‰©", "æœºåœº", "é£Ÿç‰©", "æ—¶é—´", "åœ°å›¾", "ä»˜æ¬¾"],
            "es": ["hotel", "viaje", "restaurante", "compras", "aeropuerto", "comida", "tiempo", "mapa", "pago"],
            "pt": ["hotel", "viagem", "restaurante", "compras", "aeroporto", "comida", "tempo", "mapa", "pagamento"],
            "fr": ["hÃ´tel", "voyage", "restaurant", "achats", "aÃ©roport", "repas", "temps", "carte", "paiement"],
            "id": ["hotel", "perjalanan", "restoran", "belanja", "bandara", "makanan", "waktu", "peta", "pembayaran"],
            "en": ["hotel", "travel", "restaurant", "shopping", "airport", "food", "time", "map", "payment"],
        }
        base = FALLBACKS.get(lang_code, FALLBACKS["en"])
        need = n - len(words)
        words += random.sample(base, min(need, len(base)))

    return words[:n]
    
def _gen_vocab_list_from_spec(spec: dict, lang_code: str) -> list[str]:
    """
    specï¼ˆtheme/context/pos/relation_mode/difficulty/pattern_hintï¼‰ã‚’å°Šé‡ã—ã¦èªå½™æŠ½å‡ºã€‚
    ã¾ãš spec æº–æ‹ ã§å¼·ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€è¶³ã‚Šãªã„åˆ†ã¯ exclude æŒ‡å®šã§è¿½åŠ ç”Ÿæˆã€æœ€å¾Œã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã€‚
    """
    n   = int(spec.get("count", VOCAB_WORDS))
    th  = spec.get("theme") or "general vocabulary"
    pos = spec.get("pos") or []
    rel = (spec.get("relation_mode") or "").strip().lower()
    diff = (spec.get("difficulty") or "").strip().upper()
    patt = (spec.get("pattern_hint") or "").strip()
    morph = spec.get("morphology") or []
    theme_for_prompt = translate(th, lang_code) if lang_code != "en" else th
    lang_name = LANG_NAME.get(lang_code, "the target language")
    banned = _banned_for(lang_code)

    lines = [
        f"You are selecting {n} HIGH-FREQUENCY words for the topic: {theme_for_prompt}.",
        f"Language: {lang_name}. Return ONLY one word (or short hyphenated term) per line, no numbering.",
        "No explanations. No examples. No punctuation.",
    ]
    if pos:
        lines.append("Restrict part-of-speech to: " + ", ".join(pos) + ".")
    if rel == "synonym":
        lines.append("Prefer near-synonyms around the central topic.")
    elif rel == "antonym":
        lines.append("Include at most one useful antonym if natural; otherwise skip.")
    elif rel == "collocation":
        lines.append("Prefer common collocations used in everyday speech.")
    elif rel == "pattern":
        lines.append("Prefer short reusable set phrases (but output as single tokens if possible).")
    if patt:
        lines.append(f"Pattern focus hint: {patt}.")
    if morph:
        lines.append("If natural, include related morphological family: " + ", ".join(morph) + ".")
    if diff in ("A1","A2","B1","B2"):
        lines.append(f"Approximate CEFR level: {diff}. Prefer common, practical words at this level.")
    lines.append("Avoid over-generic hotel words (check-in / reservation / receipt / lobby / elevator equivalents).")
    if lang_code in ("ko","ja","zh"):
        lines.append("Use ONLY the target script. Do not use Latin letters.")

    prompt = "\n".join(lines)

    content = ""
    for attempt in range(3):
        try:
            rsp = GPT.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role":"user","content":prompt}],
                temperature=LIST_TEMP + 0.05 * attempt,
                top_p=0.9,
            )
            content = (rsp.choices[0].message.content or "")
        except Exception:
            content = ""
        if content and content.strip():
            break

    words = _extract_words(content, lang_code, n, banned=banned)

    if len(words) < n:
        need = n - len(words)
        prompt2 = _gen_more_words_excluding(theme_for_prompt, lang_code, need, exclude=words, diff_hint=diff if diff in ("A1","A2","B1","B2") else "")
        content2 = ""
        for attempt in range(2):
            try:
                rsp2 = GPT.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[{"role":"user","content":prompt2}],
                    temperature=LIST_TEMP + 0.1 * attempt,
                    top_p=0.9,
                )
                content2 = (rsp2.choices[0].message.content or "")
            except Exception:
                content2 = ""
            if content2 and content2.strip():
                break
        more = _extract_words(content2, lang_code, need, banned=banned | set(words))
        words.extend([w for w in more if w not in words])

    if len(words) < n:
        return _gen_vocab_list(th, lang_code, n)
    return words[:n]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# æ—¥æœ¬èªTTSç”¨ãµã‚ŠãŒãªï¼ˆå˜èªãŒæ¼¢å­—ã®ã¿ã®æ™‚ï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _kana_reading(word: str) -> str:
    try:
        rsp = GPT.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{
                "role":"user",
                "content":(
                    "æ¬¡ã®æ—¥æœ¬èªå˜èªã®èª­ã¿ã‚’ã²ã‚‰ãŒãªã ã‘ã§1èªè¿”ã—ã¦ãã ã•ã„ã€‚"
                    "è¨˜å·ãƒ»æ‹¬å¼§ãƒ»èª¬æ˜ã¯ä¸è¦ã€‚\n"
                    f"å˜èª: {word}"
                )
            }],
            temperature=0.0,
            top_p=1.0,
        )
        yomi = (rsp.choices[0].message.content or "").strip()
        yomi = re.sub(r"[^ã-ã‚–ã‚ã‚ãƒ¼]+", "", yomi)
        return yomi[:20]
    except Exception:
        return ""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# å˜èªã®æ–‡è„ˆã¤ã1èªè¨³ï¼ˆå­—å¹•ç”¨ï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def translate_word_context(word: str, target_lang: str, src_lang: str, theme: str, example: str, pos_hint: str | None = None) -> str:
    theme = (theme or "").strip()
    example = (example or "").strip()
    pos_line = f"Part of speech hint: {pos_hint}." if pos_hint else ""
    prompt_lines = [
        "You are a precise bilingual lexicon generator.",
        f"Source language code: {src_lang}",
        f"Target language code: {target_lang}",
        "Task: Translate the SINGLE WORD below into exactly ONE natural target-language word that matches the intended meaning in the given context.",
        "Rules:",
        "- Output ONLY one word, no punctuation, no quotes, no explanations.",
        "- The output must be written entirely in the target language.",
        f"- Return ONLY one word in {LANG_NAME.get(target_lang,'target language')} language.",
        "- Choose the sense that fits the context (theme and example sentence).",
        "- Avoid month-name or book-title senses unless clearly indicated by context.",
        pos_line,
        "",
        f"Word: {word}",
        f"Theme: {theme}" if theme else "Theme: (none)",
        f"Example sentence for context: {example}" if example else "Example sentence for context: (none)",
    ]
    try:
        rsp = GPT.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role":"user","content":"\n".join(prompt_lines)}],
            temperature=0.0, top_p=1.0
        )
        out = (rsp.choices[0].message.content or "").strip()
        out = re.sub(r"[ï¼Œã€ã€‚.!?ï¼ï¼Ÿ]+$", "", out).strip()
        out = out.split()[0] if out else ""
        return out or word
    except Exception:
        return word

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# å†’é ­ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆNãŒ1å›ã ã‘ï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _build_intro_line(theme: str, audio_lang: str, difficulty: str | None) -> str:
    try:
        theme_local = theme if audio_lang == "en" else translate(theme, audio_lang)
    except Exception:
        theme_local = theme
    if audio_lang == "ja":
        return f"ä»Šæ—¥ã®ãƒ†ãƒ¼ãƒã¯ã€Œ{theme_local}ã€ã€‚"
    if audio_lang == "ko":
        return f"ì˜¤ëŠ˜ì˜ ì£¼ì œëŠ” {theme_local}ì…ë‹ˆë‹¤."
    if audio_lang == "zh":
        return f"ä»Šå¤©çš„ä¸»é¢˜æ˜¯{theme_local}ã€‚"
    if audio_lang == "id":
        return f"Topik hari ini: {theme_local}."
    if audio_lang == "pt":
        return f"O tema de hoje Ã© {theme_local}."
    if audio_lang == "es":
        return f"El tema de hoy es {theme_local}."
    if audio_lang == "fr":
        return f"Le thÃ¨me dâ€™aujourdâ€™hui est {theme_local}."
    return f"Today's theme: {theme_local}."

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ãã®ãƒ©ã‚¦ãƒ³ãƒ‰ã®å…¨èªã‚’ä½¿ã†çŸ­ã„ä¼šè©±ï¼ˆAlice/Bobï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _gen_conversation_using_words(words: list[str], lang_code: str, lines_per_round: int = CONVO_LINES) -> list[tuple[str, str]]:
    """
    ä¸ãˆãŸèªã‚’ã™ã¹ã¦1å›ä»¥ä¸Šä½¿ã†çŸ­ã„ä¼šè©±ã€‚
    å‡ºåŠ›ã¯ã‚»ãƒªãƒ•ã®ã¿ï¼ˆè©±è€…åãªã—ï¼‰ã‚’æƒ³å®šã—ã€ã“ã¡ã‚‰ã§ Alice/Bob ã‚’äº¤äº’ã«ä»˜ä¸ã™ã‚‹ã€‚
    ko/ja/zh ã§ã¯è‹±å­—ãŒæ··ã–ã£ãŸã‚‰å†ç”Ÿæˆã€‚
    """
    rules = _lang_rules(lang_code)
    lang_name = LANG_NAME.get(lang_code, "English")
    word_list = ", ".join(words)

    system = {
        "role": "system",
        "content": (
            "Write a short, natural two-person dialogue. "
            "Use ALL given words at least once, distributed naturally. "
            "Do NOT add speaker names or bullets. No emojis, no stage directions. "
            "Return EXACTLY the requested number of lines, one sentence per line. "
            "Keep strictly monolingual."
        ),
    }
    user = (
        f"{rules}\n"
        f"Language: {lang_name}\n"
        f"Number of lines: {lines_per_round}\n"
        f"Words to use (all of them, at least once): {word_list}\n"
        "Output: just the lines, no names, no numbering."
        + (" Do not use any Latin letters." if lang_code in ("ko","ja","zh") else "")
    )

    raw = ""
    for _ in range(5):
        try:
            rsp = GPT.chat.completions.create(
                model="gpt-4o-mini",
                messages=[system, {"role": "user", "content": user}],
                temperature=0.5, top_p=0.9,
            )
            raw = (rsp.choices[0].message.content or "").strip()
        except Exception:
            raw = ""
        if raw and _monolingual_ok(raw, lang_code):
            break

    lines = [ln.strip() for ln in (raw or "").splitlines() if ln.strip()]
    if len(lines) < lines_per_round:
        lines += (lines[-1:] * (lines_per_round - len(lines))) if lines else [""] * lines_per_round
    lines = lines[:lines_per_round]

    fixed = []
    for t in lines:
        t = _clean_strict(t)
        if not _monolingual_ok(t, lang_code):
            t = _purge_ascii_for_tts(t, lang_code)
        fixed.append(_ensure_period_for_sentence(t, lang_code))

    out: list[tuple[str, str]] = []
    for i, t in enumerate(fixed):
        spk = "Alice" if i % 2 == 0 else "Bob"
        out.append((spk, t))
    return out

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# é‡è¤‡ã‚’é¿ã‘ã¤ã¤èªå½™ã‚’é›†ã‚ã‚‹ï¼ˆãƒ©ã‚¦ãƒ³ãƒ‰ã”ã¨ã«æ–°è¦èªï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _pick_unique_words(theme: str, audio_lang: str, n: int, base_spec: dict | None, seen: set[str]) -> list[str]:
    words: list[str] = []
    attempts = 0
    while len(words) < n and attempts < 6:
        attempts += 1
        cand = _gen_vocab_list_from_spec(base_spec, audio_lang) if isinstance(base_spec, dict) else _gen_vocab_list(theme, audio_lang, n*2)
        for w in cand:
            norm = w.strip()
            if not norm:
                continue
            if audio_lang in ("ko","ja","zh") and _ASCII_LETTERS.search(norm):
                continue
            key = norm.lower() if audio_lang not in ("ja","ko","zh") else norm
            if key in seen or key in { (x.lower() if audio_lang not in ("ja","ko","zh") else x) for x in words }:
                continue
            words.append(norm)
            if len(words) >= n:
                break
    # ä¸è¶³ã‚’ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§è£œå®Œï¼ˆè¨€èªåˆ¥å®‰å…¨èªï¼‰
    if len(words) < n:
        FALLBACKS = {
            "ja": ["ãƒã‚§ãƒƒã‚¯ã‚¤ãƒ³", "äºˆç´„", "ãƒã‚§ãƒƒã‚¯ã‚¢ã‚¦ãƒˆ", "é ˜åæ›¸", "ã‚¨ãƒ¬ãƒ™ãƒ¼ã‚¿ãƒ¼", "ãƒ­ãƒ“ãƒ¼", "ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰", "é ˜åŸŸ", "æ¸…æƒ"],
            "ko": ["ì²´í¬ì¸", "ì˜ˆì•½", "ì²´í¬ì•„ì›ƒ", "ì˜ìˆ˜ì¦", "ì—˜ë¦¬ë² ì´í„°", "ë¡œë¹„", "ì—…ê·¸ë ˆì´ë“œ", "ì²­ì†Œ", "ê°ì‹¤"],
            "zh": ["åŠç†å…¥ä½", "é¢„è®¢", "é€€æˆ¿", "å‘ç¥¨", "ç”µæ¢¯", "å¤§å ‚", "å‡çº§", "æˆ¿é—´"],
            "es": ["registro", "reserva", "salida", "recibo", "ascensor", "vestÃ­bulo", "mejora"],
            "pt": ["check-in", "reserva", "checkout", "recibo", "elevador", "saguÃ£o", "upgrade"],
            "fr": ["enregistrement", "rÃ©servation", "dÃ©part", "reÃ§u", "ascenseur", "hall", "surclassement"],
            "id": ["check-in", "reservasi", "check-out", "struk", "lift", "lobi", "upgrade"],
            "en": ["check-in", "reservation", "checkout", "receipt", "elevator", "lobby", "upgrade"],
        }
        fb = FALLBACKS.get(audio_lang, FALLBACKS["en"])
        for fw in fb:
            if len(words) >= n: break
            key = fw.lower() if audio_lang not in ("ja","ko","zh") else fw
            if key not in seen and key not in { (x.lower() if audio_lang not in ("ja","ko","zh") else x) for x in words }:
                words.append(fw)
    for w in words:
        key = w.lower() if audio_lang not in ("ja","ko","zh") else w
        seen.add(key)
    return words[:n]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# éŸ³å£°ã®å˜ç´”çµåˆï¼ˆå…ˆé ­ç„¡éŸ³/æœ€çŸ­å°º/è¡Œé–“ï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _concat_with_gaps(audio_paths, gap_ms=120, pre_ms=120, min_ms=1000):
    combined = AudioSegment.silent(duration=0)
    durs = []
    for idx, p in enumerate(audio_paths):
        seg = AudioSegment.from_file(p)
        seg = AudioSegment.silent(duration=pre_ms) + seg
        if len(seg) < min_ms:
            seg += AudioSegment.silent(duration=min_ms - len(seg))
        seg_ms = len(seg)
        extra = gap_ms if idx < len(audio_paths) - 1 else 0
        combined += seg
        if extra:
            combined += AudioSegment.silent(duration=extra)
        durs.append((seg_ms + extra) / 1000.0)
    (TEMP / "full_raw.wav").unlink(missing_ok=True)
    combined.export(TEMP / "full_raw.wav", format="wav")
    return durs

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1ã‚³ãƒ³ãƒœå‡¦ç†ï¼ˆå…¨ãƒ©ã‚¦ãƒ³ãƒ‰çµ±åˆï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def run_one(topic, turns, audio_lang, subs, title_lang, yt_privacy, account, do_upload, chunk_size, context_hint="", spec=None):
    reset_temp()

    raw = (topic or "").replace("\r", "\n").strip()
    is_word_list = bool(re.search(r"[,;\n]", raw)) and len([w for w in re.split(r"[\n,;]+", raw) if w.strip()]) >= 2

    # å…¨ä½“çµ±ä¸€ã®é›£æ˜“åº¦ï¼ˆç’°å¢ƒå¤‰æ•°ã§å›ºå®š or ãƒ©ãƒ³ãƒ€ãƒ ï¼‰
    difficulty_for_all = _pick_difficulty_for_video()
    pattern_for_all = None
    pos_for_all = None

    # ãƒ†ãƒ¼ãƒ/æ–‡è„ˆ/å˜èªå–å¾—ã®æ ¹æ‹ ï¼ˆå…¨ä½“ã§çµ±ä¸€ï¼‰
    if is_word_list:
        master_theme = "custom list"
        master_context = ""
        base_spec = {"theme": master_theme, "context": master_context, "difficulty": difficulty_for_all, "count": VOCAB_WORDS}
        vocab_seed_list = [w.strip() for w in re.split(r"[\n,;]+", raw) if w.strip()]
    else:
        if isinstance(spec, dict):
            master_theme   = spec.get("theme") or topic
            master_context = (spec.get("context") or context_hint or "")
            pos_for_all    = spec.get("pos") or None
            pattern_for_all= spec.get("pattern_hint") or None
            base_spec = dict(spec)
            base_spec["count"] = VOCAB_WORDS
            base_spec["difficulty"] = (spec.get("difficulty") or difficulty_for_all).upper()
        else:
            master_theme   = topic
            master_context = context_hint or ""
            base_spec = {"theme": master_theme, "context": master_context, "difficulty": difficulty_for_all, "count": VOCAB_WORDS}

    # å†’é ­ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆNã®ã¿1å›ï¼‰
    audio_parts, sub_rows = [], [[] for _ in subs]
    plain_lines, tts_lines = [], []

    intro_line = _build_intro_line(master_theme, audio_lang, difficulty_for_all)
    intro_tts  = _ensure_period_for_sentence(intro_line, audio_lang) if audio_lang != "ja" else intro_line
    if audio_lang == "ja":
        intro_tts = normalize_ja_for_tts(intro_tts)
    if audio_lang in ("ko","ja","zh"):
        intro_tts = _purge_ascii_for_tts(intro_tts, audio_lang)
    out_audio = TEMP / f"00_intro.wav"
    speak(audio_lang, "N", intro_tts, out_audio, style=("calm" if audio_lang == "ja" else "neutral"))
    audio_parts.append(out_audio)
    plain_lines.append(intro_line)
    tts_lines.append(intro_tts)
    for r, lang in enumerate(subs):
        if lang == audio_lang:
            sub_rows[r].append(_clean_sub_line(intro_line, lang))
        else:
            try:
                sub_rows[r].append(_clean_sub_line(translate_sentence_strict(intro_line, audio_lang, lang), lang))
            except Exception:
                sub_rows[r].append(_clean_sub_line(intro_line, lang))

    # ãƒ©ã‚¦ãƒ³ãƒ‰ã”ã¨ã®å‡¦ç†
    seen_words: set[str] = set()
    round_count = VOCAB_ROUNDS

    for round_idx in range(1, round_count + 1):
        # 1) ã“ã®ãƒ©ã‚¦ãƒ³ãƒ‰ã®å˜èªã‚’æ±ºå®šï¼ˆé‡è¤‡ãªã—ï¼‰
        if is_word_list:
            pool = []
            for w in vocab_seed_list:
                key = w.lower() if audio_lang not in ("ja","ko","zh") else w
                if key not in seen_words and not (audio_lang in ("ko","ja","zh") and _ASCII_LETTERS.search(w)):
                    pool.append(w)
            if len(pool) < VOCAB_WORDS:
                # â˜… ä¿®æ­£ï¼šæ—¢å­˜ seen_words ã‚’æ¸¡ã—ã¦ã€ä»–ãƒ©ã‚¦ãƒ³ãƒ‰ã¨ã®é‡è¤‡ã‚’é˜²æ­¢
                pool.extend(_pick_unique_words(master_theme, audio_lang, VOCAB_WORDS - len(pool), base_spec, seen_words=seen_words))
            words_round = pool[:VOCAB_WORDS]
            for w in words_round:
                key = w.lower() if audio_lang not in ("ja","ko","zh") else w
                seen_words.add(key)
        else:
            words_round = _pick_unique_words(master_theme, audio_lang, VOCAB_WORDS, base_spec, seen_words)

        # ä¿é™ºï¼šã“ã®ãƒ©ã‚¦ãƒ³ãƒ‰ç¢ºå®šèªã‚’å³æ™‚ seen ã«ï¼ˆãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ã‚‚ï¼‰
        for _w in words_round:
            _key = _w.lower() if audio_lang not in ("ja","ko","zh") else _w
            if _key in seen_words:
                logging.info(f"[DEDUP] skip-dup {_w}")
            seen_words.add(_key)

        # 2) å˜èªâ†’å˜èªâ†’ä¾‹æ–‡ï¼ˆÃ—Nèªï¼‰
        round_examples: list[str] = []
        difficulty_for_this = spec.get("difficulty") if isinstance(spec, dict) else None

        for w in words_round:
            # é›£æ˜“åº¦ã‚’ä¸€ç·’ã«æ¸¡ã™ï¼ˆ_gen_example_sentence å†…ã§ CEFRãƒ¬ãƒ™ãƒ«èª¿æ•´ï¼‰
            if difficulty_for_this:
                ex = _gen_example_sentence(w, audio_lang, master_context, difficulty=difficulty_for_this)
            else:
                ex = _gen_example_sentence(w, audio_lang, master_context)
            round_examples.append(ex)

            # å˜èª
            for _rep in range(WORD_REPEAT):
                line = w
                tts_line = line
                if audio_lang == "ja":
                    if _KANJI_ONLY.fullmatch(line):
                        yomi = _kana_reading(line)
                        if yomi:
                            tts_line = yomi
                    base = re.sub(r"[ã€‚ï¼ï¼Ÿ!?]+$", "", tts_line).strip()
                    tts_line = base + ("ã€‚" if len(base) >= 2 else "")
                    tts_line = normalize_ja_for_tts(tts_line)
                else:
                    tts_line = _ensure_period_for_sentence(tts_line, audio_lang)
                if audio_lang in ("ko","ja","zh"):
                    tts_line = _purge_ascii_for_tts(tts_line, audio_lang)
                else:
                    tts_line = _clean_non_english_ascii(tts_line, audio_lang)

                out_audio = TEMP / f"{len(audio_parts)+1:02d}.wav"
                style_for_tts = "neutral"
                speak(audio_lang, "N", tts_line, out_audio, style=style_for_tts)
                audio_parts.append(out_audio)
                plain_lines.append(line)
                tts_lines.append(tts_line)

                # å­—å¹•
                for r, lang in enumerate(subs):
                    if lang == audio_lang:
                        sub_rows[r].append(_clean_sub_line(line, lang))
                    else:
                        try:
                            pos_hint = None
                            if isinstance(base_spec, dict) and base_spec.get("pos"):
                                pos_hint = ",".join(base_spec["pos"])
                            elif audio_lang == "ja":
                                _k = _guess_ja_pos(line)
                                pos_map = {"verb":"verb", "iadj":"adjective", "naadj":"adjective", "noun":"noun"}
                                pos_hint = pos_map.get(_k, None)
                            trans = translate_word_context(
                                word=line, target_lang=lang, src_lang=audio_lang,
                                theme=master_theme, example=ex, pos_hint=pos_hint
                            )
                        except Exception:
                            trans = line
                        sub_rows[r].append(_clean_sub_line(trans, lang))

            # ä¾‹æ–‡ï¼ˆ1æ–‡ï¼‰
            line = ex
            if audio_lang == "ja":
                tts_line = _PARENS_JA.sub(" ", line).strip()
                tts_line = _ensure_period_for_sentence(tts_line, audio_lang)
                tts_line = normalize_ja_for_tts(tts_line)
                style_for_tts = "calm"
            else:
                tts_line = _ensure_period_for_sentence(line, audio_lang)
                style_for_tts = "calm"

            if audio_lang in ("ko","ja","zh"):
                tts_line = _purge_ascii_for_tts(tts_line, audio_lang)
            else:
                tts_line = _clean_non_english_ascii(tts_line, audio_lang)

            out_audio = TEMP / f"{len(audio_parts)+1:02d}.wav"
            speak(audio_lang, "N", tts_line, out_audio, style=style_for_tts)
            audio_parts.append(out_audio)
            plain_lines.append(line)
            tts_lines.append(tts_line)
            for r, lang in enumerate(subs):
                if lang == audio_lang:
                    sub_rows[r].append(_clean_sub_line(line, lang))
                else:
                    try:
                        trans = translate_sentence_strict(line, src_lang=audio_lang, target_lang=lang)
                    except Exception:
                        trans = line
                    sub_rows[r].append(_clean_sub_line(trans, lang))

        # 3) ã¾ã¨ã‚ä¼šè©±ï¼ˆã“ã®ãƒ©ã‚¦ãƒ³ãƒ‰ã®èªã‚’å…¨éƒ¨ä½¿ã†ï¼‰
        if not NO_CONVO:
            convo = _gen_conversation_using_words(words_round, audio_lang, lines_per_round=CONVO_LINES)
            for spk, line in convo:
                if audio_lang == "ja":
                    base = re.sub(r"[ã€‚ï¼ï¼Ÿ!?]+$", "", line).strip()
                    tts_line = base + ("ã€‚" if base and base[-1] not in "ã€‚ï¼ï¼Ÿ!?" else "")
                    tts_line = normalize_ja_for_tts(tts_line)
                else:
                    tts_line = _ensure_period_for_sentence(line, audio_lang)
                if audio_lang in ("ko","ja","zh"):
                    tts_line = _purge_ascii_for_tts(tts_line, audio_lang)
                else:
                    tts_line = _clean_non_english_ascii(tts_line, audio_lang)

                out_audio = TEMP / f"{len(audio_parts)+1:02d}.wav"
                speak(audio_lang, spk, tts_line, out_audio, style=("calm" if audio_lang == "ja" else "neutral"))
                audio_parts.append(out_audio)
                plain_lines.append(line)
                tts_lines.append(tts_line)
                for r, lang in enumerate(subs):
                    if lang == audio_lang:
                        sub_rows[r].append(_clean_sub_line(line, lang))
                    else:
                        try:
                            trans = translate_sentence_strict(line, src_lang=audio_lang, target_lang=lang)
                        except Exception:
                            trans = line
                        sub_rows[r].append(_clean_sub_line(trans, lang))
                                        
    # â”€â”€ å˜ç´”çµåˆ â†’ æ•´éŸ³ â†’ mp3 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    gap_ms = GAP_MS_JA if audio_lang == "ja" else GAP_MS
    pre_ms = PRE_SIL_MS_JA if audio_lang == "ja" else PRE_SIL_MS
    min_ms = MIN_UTTER_MS_JA if audio_lang == "ja" else MIN_UTTER_MS

    new_durs = _concat_with_gaps(audio_parts, gap_ms=gap_ms, pre_ms=pre_ms, min_ms=min_ms)
    enhance(TEMP/"full_raw.wav", TEMP/"full.wav")
    AudioSegment.from_file(TEMP/"full.wav").export(TEMP/"full.mp3", format="mp3")

    # èƒŒæ™¯ç”»åƒï¼ˆãƒ†ãƒ¼ãƒè‹±è¨³ã§æ¤œç´¢ï¼‰
    bg_png = TEMP / "bg.png"
    try:
        theme_en = translate(master_theme, "en")
    except Exception:
        theme_en = master_theme
    fetch_bg(theme_en or "learning", bg_png)

    # lines.jsonï¼ˆå†’é ­ã‚¿ã‚¤ãƒˆãƒ«ï¼‹å…¨ãƒ©ã‚¦ãƒ³ãƒ‰ï¼‰
    lines_data = []
    for i, dur in enumerate(new_durs):
        row = ["N"]
        for r in range(len(subs)):
            row.append(sub_rows[r][i])
        row.append(dur)
        lines_data.append(row)
    (TEMP/"lines.json").write_text(json.dumps(lines_data, ensure_ascii=False, indent=2), encoding="utf-8")

    # ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
    try:
        (TEMP / "script_raw.txt").write_text("\n".join(plain_lines), encoding="utf-8")
        (TEMP / "script_tts.txt").write_text("\n".join(tts_lines), encoding="utf-8")
        with open(TEMP / "subs_table.tsv", "w", encoding="utf-8") as f:
            header = ["idx", "text"] + [f"sub:{code}" for code in subs]
            f.write("\t".join(header) + "\n")
            for idx_tbl in range(len(plain_lines)):
                row = [str(idx_tbl+1), _clean_sub_line(plain_lines[idx_tbl], audio_lang)]
                for r in range(len(subs)):
                    row.append(sub_rows[r][idx_tbl])
                f.write("\t".join(row) + "\n")
        with open(TEMP / "durations.txt", "w", encoding="utf-8") as f:
            total = 0.0
            for i, d in enumerate(new_durs, 1):
                total += d
                f.write(f"{i:02d}\t{d:.3f}s\n")
            f.write(f"TOTAL\t{total:.3f}s\n")
        # ä¾‹æ–‡ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯çµ±è¨ˆ
        with open(TEMP / "fallback_stats.json", "w", encoding="utf-8") as f:
            json.dump(FALLBACK_STATS, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logging.warning(f"[DEBUG_SCRIPT] write failed: {e}")

    if args.lines_only:
        return

    # ã‚µãƒ ãƒ
    thumb = TEMP / "thumbnail.jpg"
    thumb_lang = subs[1] if len(subs) > 1 else audio_lang
    make_thumbnail(master_theme, thumb_lang, thumb)

    # å‹•ç”»ç”Ÿæˆï¼ˆæ¨ªå‘ã16:9 / coverï¼‰
    stamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    final_mp4 = OUTPUT / f"{audio_lang}-{'_'.join(subs)}_{stamp}.mp4"
    final_mp4.parent.mkdir(parents=True, exist_ok=True)

    cmd = [
        "python", str(BASE/"chunk_builder.py"),
        str(TEMP/"lines.json"), str(TEMP/"full.mp3"), str(bg_png),
        "--chunk", str(chunk_size),
        "--rows", str(len(subs)),
        "--out", str(final_mp4),
        "--center-n",
    ]
    logging.info("ğŸ”¹ chunk_builder cmd: %s", " ".join(cmd))
    subprocess.run(cmd, check=True)

    if not do_upload:
        return

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ãƒ¡ã‚¿ç”Ÿæˆï¼†ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    INCLUDE_VOCAB_IN_TITLE = os.getenv("TITLE_VOCAB", "0") == "1"
    SERIES_LABELS = {
        "en": "Real Practice Series",
        "ja": "Real Practice ã‚·ãƒªãƒ¼ã‚º",
        "ko": "Real Practice ì‹œë¦¬ì¦ˆ",
        "zh": "Real Practice ç³»åˆ—",
        "es": "Serie Real Practice",
        "pt": "SÃ©rie Real Practice",
        "fr": "SÃ©rie Real Practice",
        "id": "Seri Real Practice",
    }

    def make_title(theme, title_lang: str, audio_lang_for_label: str | None = None,
                   pos: list[str] | None = None, difficulty: str | None = None,
                   pattern_hint: str | None = None):
        """
        å½¢å¼ï¼šã€Œ<ç¿»è¨³æ¸ˆã¿Topic>[ èªå½™/Vocabulary] | <ã‚·ãƒªãƒ¼ã‚ºå> (A2)ã€
        â€œè¦‹åˆ‡ã‚Œå¯¾ç­–ã®æ‰‹å‹•ã‚¯ãƒªãƒƒãƒ—â€ã¯å»ƒæ­¢ã€‚100æ–‡å­—ä¸Šé™ã ã‘ sanitize_title() ã«å§”ã­ã‚‹ã€‚
        """
        level = (difficulty or "A2").upper()
        series_name = SERIES_LABELS.get(title_lang, "Real Practice Series")
        try:
            theme_local = theme if title_lang == "en" else translate(theme, title_lang)
        except Exception:
            theme_local = theme

        if title_lang == "ja":
            topic_part = f"{theme_local} èªå½™" if INCLUDE_VOCAB_IN_TITLE else f"{theme_local}"
        else:
            topic_part = f"{theme_local} Vocabulary" if INCLUDE_VOCAB_IN_TITLE else f"{theme_local}"

        title_raw = f"{topic_part} | {series_name} ({level})"
        return sanitize_title(title_raw)

    def make_desc(theme, title_lang: str):
        if title_lang not in LANG_NAME:
            title_lang = "en"
        try:
            theme_local = theme if title_lang == "en" else translate(theme, title_lang)
        except Exception:
            theme_local = theme
        msg = {
            "ja": f"{theme_local} ã«å¿…é ˆã®èªå½™ã‚’çŸ­æ™‚é–“ã§ãƒã‚§ãƒƒã‚¯ã€‚å£°ã«å‡ºã—ã¦ä¸€ç·’ã«ç·´ç¿’ã—ã‚ˆã†ï¼ #vocab #learning",
            "en": f"Quick practice for {theme_local} vocabulary. Repeat after the audio! #vocab #learning",
            "pt": f"Pratique rÃ¡pido o vocabulÃ¡rio de {theme_local}. Repita em voz alta! #vocab #aprendizado",
            "es": f"PrÃ¡ctica rÃ¡pida de vocabulario de {theme_local}. Â¡Repite en voz alta! #vocab #aprendizaje",
            "ko": f"{theme_local} ì–´íœ˜ë¥¼ ë¹ ë¥´ê²Œ ì—°ìŠµí•˜ì„¸ìš”. ì†Œë¦¬ ë‚´ì–´ ë”°ë¼ ë§í•´ìš”! #vocab #learning",
            "id": f"Latihan cepat kosakata {theme_local}. Ucapkan keras-keras! #vocab #belajar",
            "fr": f"EntraÃ®nement rapide du vocabulaire {theme_local}. RÃ©pÃ©tez Ã  voix haute ! #vocab #apprentissage",
            "zh": f"å¿«é€Ÿç»ƒä¹  {theme_local} è¯æ±‡ã€‚è·Ÿç€éŸ³é¢‘å¤§å£°ç»ƒä¹ ï¼ #vocab #learning",
        }
        return msg.get(title_lang, msg["en"])

    def make_tags(theme, audio_lang, subs, title_lang, difficulty=None, pos=None):
        tags = [
            theme, "vocabulary", "language learning", "speaking practice",
            "listening practice", "subtitles"
        ]
        if difficulty:
            tags.append(f"CEFR {difficulty}")
        if pos:
            for p in pos:
                tags.append(p)
        for code in subs:
            if code in LANG_NAME:
                tags.append(f"{LANG_NAME[code]} subtitles")
        seen_t, out = set(), []
        for t in tags:
            if t not in seen_t:
                seen_t.add(t)
                out.append(t)
        return out[:15]

    pos_for_title = pos_for_all
    difficulty_for_title = difficulty_for_all
    pattern_for_title = pattern_for_all

    title = make_title(
        master_theme, title_lang, audio_lang_for_label=audio_lang,
        pos=pos_for_title, difficulty=difficulty_for_title, pattern_hint=pattern_for_title
    )
    desc  = make_desc(master_theme, title_lang)
    tags  = make_tags(master_theme, audio_lang, subs, title_lang,
                      difficulty=difficulty_for_title, pos=pos_for_title)

    def _is_limit_error(err: Exception) -> bool:
        s = str(err)
        return ("uploadLimitExceeded" in s or "quotaExceeded" in s or
                "The user has exceeded the number of videos they may upload" in s)

    def _is_token_error(err: Exception) -> bool:
        s = str(err).lower()
        return ("token" in s and "expired" in s) or ("invalid_grant" in s) or ("unauthorized_client" in s) or ("invalid_credentials" in s) or ("401" in s)

    def _try_upload_with_fallbacks() -> bool:
        fb = os.getenv("UPLOAD_FALLBACKS", "").strip()
        fallbacks = [x.strip() for x in fb.split(",") if x.strip()]
        tried = []
        for acc in [account] + [a for a in fallbacks if a != account]:
            tried.append(acc)
            try:
                upload(
                    video_path=final_mp4, title=title, desc=desc, tags=tags,
                    privacy=yt_privacy, account=acc, thumbnail=thumb, default_lang=audio_lang
                )
                logging.info(f"[UPLOAD] âœ… success on account='{acc}'")
                return True
            except Exception as e:
                if _is_limit_error(e):
                    logging.warning(f"[UPLOAD] âš ï¸ limit reached on account='{acc}' â†’ trying next fallback.")
                    continue
                if _is_token_error(e):
                    logging.error(f"[UPLOAD] âŒ TOKEN ERROR on account='{acc}'")
                    try:
                        (TEMP / "TOKEN_EXPIRED.txt").write_text(
                            f"Token expired or revoked for account='{acc}'.\nDetail:\n{e}",
                            encoding="utf-8",
                        )
                    except Exception:
                        pass
                    raise SystemExit(f"[ABORT] Token expired/revoked for account='{acc}'. Please reauthorize.")
                logging.exception(f"[UPLOAD] unexpected error on account='{acc}'")
                raise
        msg = f"Upload skipped due to per-account limits. tried={tried}"
        logging.warning("[UPLOAD] " + msg)
        try:
            (TEMP / "UPLOAD_SKIPPED.txt").write_text(msg, encoding="utf-8")
        except Exception:
            pass
        return False

    _try_upload_with_fallbacks()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def run_all(topic, turns, privacy, do_upload, chunk_size):
    isolated = os.getenv("ISOLATED_RUN", "0") == "1"
    if isolated:
        for combo in COMBOS:
            audio_lang  = combo["audio"]
            subs        = combo["subs"]
            account     = combo.get("account","default")
            title_lang  = _infer_title_lang(audio_lang, subs, combo)

            if TARGET_ONLY and account != TARGET_ONLY:
                continue

            picked_topic = topic
            context_hint = ""
            spec_for_run = None

            if topic.strip().lower() == "auto":
                try:
                    picked_raw = pick_by_content_type("vocab", audio_lang, return_context=True)
                    if isinstance(picked_raw, dict):
                        picked_topic = picked_raw.get("theme") or "general vocabulary"
                        context_hint = picked_raw.get("context") or ""
                        spec_for_run = dict(picked_raw)
                        spec_for_run["count"] = VOCAB_WORDS
                    elif isinstance(picked_raw, tuple) and len(picked_raw) == 2:
                        picked_topic, context_hint = picked_raw[0], picked_raw[1]
                        spec_for_run = {"theme": picked_topic, "context": context_hint, "count": VOCAB_WORDS}
                    else:
                        picked_topic = str(picked_raw)
                        spec_for_run = {"theme": picked_topic, "context": "", "count": VOCAB_WORDS}
                except TypeError:
                    picked_raw = pick_by_content_type("vocab", audio_lang)
                    picked_topic = picked_raw if isinstance(picked_raw, str) else str(picked_raw)
                    spec_for_run = {"theme": picked_topic, "context": "", "count": VOCAB_WORDS}

            logging.info(f"[ISOLATED] {audio_lang} | subs={subs} | account={account} | theme={picked_topic}")
            run_one(
                picked_topic, turns, audio_lang, subs, title_lang,
                privacy, account, do_upload, chunk_size,
                context_hint=context_hint, spec=spec_for_run
            )
        return

    for combo in COMBOS:
        account = combo.get("account", "default")
        if TARGET_ONLY and account != TARGET_ONLY:
            continue
        cmd = [
            sys.executable, str(BASE / "main.py"),
            topic,
            "--privacy", privacy,
            "--chunk", str(chunk_size),
            "--account", account,
        ]
        if not do_upload:
            cmd.append("--no-upload")
        env = os.environ.copy()
        env["ISOLATED_RUN"] = "1"
        logging.info(f"â–¶ Spawning isolated run for account={account}: {' '.join(cmd)}")
        subprocess.run(cmd, check=False, env=env)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")
    ap = argparse.ArgumentParser()
    ap.add_argument("topic", help="èªå½™ãƒ†ãƒ¼ãƒã€‚AUTO ã§è‡ªå‹•é¸æŠã€‚ã‚«ãƒ³ãƒ/æ”¹è¡ŒåŒºåˆ‡ã‚Šãªã‚‰å˜èªãƒªã‚¹ãƒˆã¨ã—ã¦ä½¿ç”¨")
    ap.add_argument("--turns", type=int, default=8)  # äº’æ›ç”¨
    ap.add_argument("--privacy", default="unlisted", choices=["public","unlisted","private"])
    ap.add_argument("--lines-only", action="store_true")
    ap.add_argument("--no-upload", action="store_true")
    ap.add_argument("--chunk", type=int, default=9999, help="Longå‹•ç”»ã¯åˆ†å‰²ã›ãš1æœ¬ã§OK")
    ap.add_argument("--account", type=str, default="", help="ã“ã® account ã®ã¿å®Ÿè¡Œï¼ˆcombos.yaml ã® account å€¤ã«ä¸€è‡´ï¼‰")
    args = ap.parse_args()

    target_cli = (args.account or "").strip()
    target_env = os.getenv("TARGET_ACCOUNT", "").strip()
    TARGET_ONLY = target_cli or target_env

    if TARGET_ONLY:
        selected = [c for c in COMBOS if c.get("account", "default") == TARGET_ONLY]
        if not selected:
            logging.error(f"[ABORT] No combos matched account='{TARGET_ONLY}'. Check combos.yaml.")
            raise SystemExit(2)
        COMBOS[:] = selected
        logging.info(f"[ACCOUNT FILTER] Running only for account='{TARGET_ONLY}' ({len(COMBOS)} combo(s)).")

    topic = resolve_topic(args.topic)
    run_all(topic, args.turns, args.privacy, not args.no_upload, args.chunk)
