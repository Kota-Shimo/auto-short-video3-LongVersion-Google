#!/usr/bin/env python
"""
main.py â€“ VOCABå°‚ç”¨ç‰ˆï¼ˆå˜ç´”çµåˆï¼‹æ—¥æœ¬èªãµã‚ŠãŒãª[TTSã®ã¿]ï¼‹å…ˆé ­ç„¡éŸ³ï¼‹æœ€çŸ­1ç§’ï¼‰

æ”¹å–„ç‚¹:
- ä¾‹æ–‡ã¯å¥èª­ç‚¹ä»˜ãè‡ªç„¶æ–‡ã®ã¿ç”Ÿæˆï¼ˆè‹±å­—ãƒ»è¨˜å·é™¤å»ãƒ»å†ç”Ÿæˆã‚ã‚Šï¼‰
- å˜èªè¡Œã«å¥ç‚¹ã‚’ä»˜ã‘ãªã„ï¼ˆã‚¤ãƒ³ãƒˆãƒãƒ¼ã‚·ãƒ§ãƒ³å´©ã‚Œé˜²æ­¢ï¼‰
- ENHANCE_DISABLE=1 ã§ãƒã‚¹ãƒ—ãƒ­ç„¡åŠ¹åŒ–ãƒ†ã‚¹ãƒˆå¯èƒ½
"""

import argparse, logging, re, json, subprocess, os
from datetime import datetime
from shutil import rmtree
from pathlib import Path
import yaml
from pydub import AudioSegment
from openai import OpenAI

from config import BASE, OUTPUT, TEMP
from translate import translate
from tts_openai import speak
from audio_fx import enhance
from bg_image import fetch as fetch_bg
from thumbnail import make_thumbnail
from upload_youtube import upload
from topic_picker import pick_by_content_type

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
GPT = OpenAI()
DEBUG_SCRIPT = os.getenv("DEBUG_SCRIPT", "0") == "1"
GAP_MS = int(os.getenv("GAP_MS", "120"))
PRE_SIL_MS = int(os.getenv("PRE_SIL_MS", "120"))
MIN_UTTER_MS = int(os.getenv("MIN_UTTER_MS", "1000"))
ENHANCE_DISABLE = os.getenv("ENHANCE_DISABLE", "0") == "1"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with open(BASE / "combos.yaml", encoding="utf-8") as f:
    COMBOS = yaml.safe_load(f)["combos"]

LANG_NAME = {
    "en": "English", "pt": "Portuguese", "id": "Indonesian",
    "ja": "Japanese", "ko": "Korean", "es": "Spanish",
}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ä¾‹æ–‡ç”Ÿæˆï¼ˆå³æ ¼åŒ–ï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _gen_example_sentence(word: str, lang_code: str) -> str:
    """å˜èªã‚’ä½¿ã£ãŸè‡ªç„¶ãªçŸ­æ–‡ã‚’ç”Ÿæˆï¼ˆå¥èª­ç‚¹ä»˜ãã€è‹±å­—ãƒ»è¨˜å·å‰Šé™¤ï¼‰"""
    lang = LANG_NAME.get(lang_code, "Japanese")
    prompt = (
        f"Write one short, natural sentence in {lang} using the word '{word}'.\n"
        "Rules:\n"
        "- Must sound natural and complete.\n"
        "- Use only the target language script (for Japanese: ã²ã‚‰ãŒãªãƒ»ã‚«ã‚¿ã‚«ãƒŠãƒ»å¸¸ç”¨æ¼¢å­—)\n"
        "- No English letters, emojis, or brackets.\n"
        "- End with a full stop (for Japanese: ã€‚)\n"
        "- Return only the sentence."
    )
    for _ in range(2):  # æœ€å¤§2å›ã¾ã§å†è©¦è¡Œ
        try:
            rsp = GPT.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.4,
            )
            sent = (rsp.choices[0].message.content or "").strip()
            sent = re.sub(r'[\"â€œâ€\'\[\]\(\){}<>/\\|~^_=+*#@ğŸ™‚-ğŸ™ƒ0-9]+', '', sent)
            sent = re.sub(r"\s+", " ", sent)
            sent = sent.strip()
            if lang_code == "ja" and not sent.endswith("ã€‚"):
                sent += "ã€‚"
            if 5 <= len(sent) <= 40:
                return sent
        except Exception:
            continue
    return f"{word} ã‚’ä½¿ã£ã¦ç·´ç¿’ã—ã¾ã—ã‚‡ã†ã€‚"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ãµã‚ŠãŒãªå‡¦ç†
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_KANJI_ONLY = re.compile(r"^[ä¸€-é¾¥ã€…]+$")
def _kana_reading(word: str) -> str:
    try:
        rsp = GPT.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user",
                       "content": f"æ¬¡ã®æ—¥æœ¬èªå˜èªã®èª­ã¿ã‚’ã²ã‚‰ãŒãªã§1èªã ã‘è¿”ã—ã¦ãã ã•ã„ã€‚\nå˜èª: {word}"}],
            temperature=0.0,
        )
        yomi = re.sub(r"[^ã-ã‚–ã‚ã‚ãƒ¼]", "", (rsp.choices[0].message.content or ""))
        return yomi[:20]
    except Exception:
        return ""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# éŸ³å£°ã®å˜ç´”çµåˆ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _concat_with_gaps(audio_paths, gap_ms=120, pre_ms=120, min_ms=1000):
    combined = AudioSegment.silent(duration=0)
    durs = []
    for idx, path in enumerate(audio_paths):
        seg = AudioSegment.from_file(path)
        seg = AudioSegment.silent(duration=pre_ms) + seg
        if len(seg) < min_ms:
            seg += AudioSegment.silent(duration=min_ms - len(seg))
        combined += seg
        if idx < len(audio_paths) - 1:
            combined += AudioSegment.silent(duration=gap_ms)
        durs.append((len(seg) + (gap_ms if idx < len(audio_paths)-1 else 0)) / 1000)
    combined.export(TEMP / "full_raw.wav", format="wav")
    return durs

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1ã‚³ãƒ³ãƒœå®Ÿè¡Œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def run_one(topic, turns, audio_lang, subs, title_lang, yt_privacy, account, do_upload, chunk_size):
    from translate import translate
    from audio_fx import enhance
    reset_temp()

    raw = (topic or "").replace("\r", "\n").strip()
    words_count = int(os.getenv("VOCAB_WORDS", "6"))
    vocab_words = [w.strip() for w in re.split(r"[\n,;]+", raw) if w.strip()]
    if not vocab_words:
        vocab_words = ["check-in", "reservation", "checkout", "lobby", "upgrade", "receipt"]

    dialogue = []
    for w in vocab_words:
        ex = _gen_example_sentence(w, audio_lang)
        dialogue.extend([("N", w), ("N", w), ("N", ex)])

    audio_parts, sub_rows = [], [[] for _ in subs]
    plain_lines, tts_lines = [], []

    for i, (spk, line) in enumerate(dialogue, 1):
        block_pos = (i - 1) % 3  # 0:å˜èª1, 1:å˜èª2, 2:ä¾‹æ–‡
        tts_line = line
        if audio_lang == "ja" and _KANJI_ONLY.fullmatch(line):
            yomi = _kana_reading(line)
            if yomi:
                tts_line = yomi
        # ä¾‹æ–‡è¡Œã®ã¿å¥ç‚¹ã‚’è¿½åŠ 
        if audio_lang == "ja" and block_pos == 2 and not re.search(r"[ã€‚!?ï¼ï¼Ÿ]$", tts_line):
            tts_line += "ã€‚"

        out_audio = TEMP / f"{i:02d}.wav"
        speak(audio_lang, spk, tts_line, out_audio, style="neutral")
        audio_parts.append(out_audio)
        plain_lines.append(line)
        tts_lines.append(tts_line)

        # å­—å¹•
        for r, lang in enumerate(subs):
            sub_rows[r].append(line if lang == audio_lang else translate(line, lang))

    # éŸ³å£°é€£çµãƒ»ã‚¨ãƒ³ãƒãƒ³ã‚¹
    durs = _concat_with_gaps(audio_parts, gap_ms=GAP_MS, pre_ms=PRE_SIL_MS, min_ms=MIN_UTTER_MS)
    if ENHANCE_DISABLE:
        AudioSegment.from_file(TEMP/"full_raw.wav").export(TEMP/"full.mp3", format="mp3")
    else:
        enhance(TEMP/"full_raw.wav", TEMP/"full.wav")
        AudioSegment.from_file(TEMP/"full.wav").export(TEMP/"full.mp3", format="mp3")

    # ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
    if DEBUG_SCRIPT:
        (TEMP/"script_raw.txt").write_text("\n".join(plain_lines), encoding="utf-8")
        (TEMP/"script_tts.txt").write_text("\n".join(tts_lines), encoding="utf-8")

    # lines.json å‡ºåŠ›
    lines = []
    for (spk, txt), dur in zip(dialogue, durs):
        row = [spk] + [sub_rows[r][len(lines)] for r in range(len(subs))] + [dur]
        lines.append(row)
    (TEMP/"lines.json").write_text(json.dumps(lines, ensure_ascii=False, indent=2), encoding="utf-8")

    # èƒŒæ™¯
    bg_png = TEMP / "bg.png"
    fetch_bg(vocab_words[0], bg_png)

    # å‹•ç”»ç”Ÿæˆ
    stamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    out_mp4 = OUTPUT / f"{audio_lang}-{'_'.join(subs)}_{stamp}.mp4"
    subprocess.run([
        "python", str(BASE/"chunk_builder.py"),
        str(TEMP/"lines.json"), str(TEMP/"full.mp3"), str(bg_png),
        "--chunk", str(chunk_size), "--rows", str(len(subs)), "--out", str(out_mp4)
    ], check=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def reset_temp():
    if TEMP.exists(): rmtree(TEMP)
    TEMP.mkdir(exist_ok=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    ap = argparse.ArgumentParser()
    ap.add_argument("topic")
    ap.add_argument("--no-upload", action="store_true")
    args = ap.parse_args()
    run_one(args.topic, 0, "ja", ["ja","en"], "ja", "unlisted", "default", not args.no_upload, 9999)